---
title: 'JHU/Coursera Practical Machine Learning: Course Project'
author: "Iraklis S Konstantopoulos"
date: "18 August 2015"
output: html_document
---

# Data input
The data I will use in this project were collected and made public by Wallace Ugulino, Eduardo Velloso, and Hugo Fuks [^1]. Read more about this Human Activity Recognition experiment at 
<http://groupware.les.inf.puc-rio.br/har> or see the related publication: 

[^1]: Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 

```{r, warning=FALSE}
doMC::registerDoMC(cores=4) # Enable multi-core processing

url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

set_train <- read.csv(url(url_train, method="libcurl"))
set_test <- read.csv(url(url_test, method="libcurl"))
```

### Cleaning: Missing Values and Useful Predictors
A quick glance at the dataset reveals that many columns include few observations, some of which are actually empty strings rather than NA. I will rectify this by converting `#DIV/0` and `""` to `NA`, and then count up the fractional occurrence of `NA` in each column.
```{r, warning=FALSE}
set_train[set_train==""] <- NA; set_train[set_train=="#DIV/0!"] <- NA
set_test[set_test==""] <- NA; set_test[set_test=="#DIV/0!"] <- NA

nCols <- dim(set_train)[2]
fracNA <- numeric(length = nCols) 
for(i in 1:nCols){
  fracNA[i] = sum(is.na(set_train[i]))/dim(set_train)[1]
}
print(levels(factor(fracNA)))
```
That is, columns can be split into two subsets: one with no missing values, and another where at least 98% of values are `NA`. The latter cannot give rise to reliable predictors and hence should be dropped from both imported datasets. 

```{r, warning=FALSE}
set_train <- set_train[fracNA==0]
set_test <- set_test[fracNA==0]
```

### Superfluous Covariates
A final step in preprocessing the dataset is to examine the variability of each parameter in the data frame and exclude those that do not contribute to the variabililty. Through the `nzv` function we can identify near zero variance predictors and exclude them from the training and test sets. 

```{r}
library(lattice); library(ggplot2); library(caret)
nzv <- nearZeroVar(set_train,saveMetrics=TRUE)
# Drop all predictors where nzv==TRUE
print(names(set_train[nzv$nzv==TRUE]))
```

The above indicates that the `new_window` predictor does not vary and should be dropped. In addition, the timestamp does not record a useful variable (easy to assess by plotting it against the classe outcome variable), and the `X` variable is just an index that might confuse the model. 
```{r, echo=FALSE}
set_train$new_window = NULL
set_train$X = NULL
set_train$cvtd_timestamp = NULL
set_train$raw_timestamp_part_1 = NULL
set_train$raw_timestamp_part_2 = NULL

set_test$new_window = NULL
set_test$X = NULL
set_test$cvtd_timestamp = NULL
set_test$raw_timestamp_part_1 = NULL
set_test$raw_timestamp_part_2 = NULL
```

```{r}
rbind(train=dim(set_train), test=dim(set_test))
```
This exercise reduced the dimensionality of the dataset to a much more manageable 55 columns. This is particularly important since this is not a linear problem. Instead I will be classifying with trees, a process that is prone to overfitting when a plethora of predictors are available. 


# Model setup
In order to set up a reasonable machine learning algorithm to infer the type of exercise being performed I will follow these steps: 

1. Preprocess both imported datasets identically by turning string variables to numbers

1. Cross-validate the training set, facilitated by the abundance of data

1. Develop a classification tree using a random forest

1. Estimate the out-of-sample error of our categorical dataset through the accuracy and conconrdance

1. Make predictions on the imported `set_test` set

### Processing and Cross-Validation
All variables are numeric or represented as factors. The large size of the available training set allows for a thorough cross-validation procedure. Given the abundance of data I will set 10% of the imported training set aside as a validation set, instead of using an automated, say $k$-folds, process, as stochastic effects can be expected to be minimal. 
```{r, warning=FALSE}
set.seed(13311)
validate <- createDataPartition(y=set_train$classe, p=0.1, list=FALSE)
training <- set_train[-validate,]
validation <- set_train[validate,]
```

### Model Construction
With my validation set aside I can now set up a classification tree model to understand how the `classe` variable (subject's action) correlates with every other one in the data set. This will be the most computationally intensive part of the process, given the nearly 2e5 obesrvations. 
```{r, warning=FALSE}
library(rpart)
myTreeRpart <- train(classe ~ ., method='rpart', data=training)
```

How well did this model perform? We can quantify this as the proportion of correct classificaitons as a tabular schematic:
```{r, warning=FALSE}
sum(predict(myTreeRpart, validation) == validation$classe)/dim(validation)[1]
table(predict(myTreeRpart, validation), validation$classe)
```

A success rate of 57% is not high enough, so this calls for a more robust algorithm. I will use a random forest instead of a generic classification tree. 
```{r, warning=FALSE, cache=TRUE}
library(randomForest)
myTree <- randomForest(classe ~ ., data=training)
```

To validate the effectiveness of this algorithm I will predict `classe` in every observation of the validation set (to test for overfitting), and then the imported `set_test` data. 
```{r, warning=FALSE}
sum(predict(myTree, newdata=validation) == validation$classe) / dim(validation)[1]
```

The success rate in classifying the validation set is nearly 100%. 

### Constraining Errors
In this case we have a categorical variable, rather than a continuous one, so the appropriate measures of error are the Accuracy and Conconrdance (denoted as 'Kappa' in the following output), which we can obtain through a confusion matrix: 
```{r}
confusionMatrix(validation$classe,predict(myTree,validation))
```

The formal calculation of Accuracy and Concordance confirm the precision with which the algorithm classifies the `classe` variable. 

# Predicting the Test Set
The last step is to use the model in order to predict which action (`classe`) is being undertaken by the test subject in each of 20 instances in the imported `set_test`. 
```{r, warning=FALSE, eval=FALSE}
predict(myTree, newdata=set_test)
```

I have not output the results here but suffice it to report that all five classes are represented here, which lends confidence to the validity of the model. 

The script provided by the instructors produces a suite of files for submitting results. I will not echo this code while I output results files (see Rmd file). 
```{r,  warning=FALSE, echo=FALSE}
answers <- as.character(predict(myTree, set_test))

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```
